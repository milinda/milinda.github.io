---
type: post
layout: post
blog: true
title: "The Dataflow Model: A Practical Approach to Balancing Correctness, Latency, and Cost in Massive-Scale, Unbounded, Out-of-Order Data Processing"
categories: research
tags: dataflow, stream-processing, streaming-sql
---

**[The Dataflow Model: A Practical Approach to Balancing Correctness, Latency, and Cost in Massive-Scale, Unbounded, Out-of-Order Data Processing](http://www.vldb.org/pvldb/vol8/p1792-Akidau.pdf)** Akidau et al., *VLDB 2015* 

As mentioned [here](http://www.infoworld.com/article/2608040/big-data/fast-data--the-next-step-after-big-data.html), Big Data is not just big; it's also fast. Big data is often generated at incredible rates from sources such as click-streams, remote sensors and logs generated by web services. At the Internet scale these data sources are often unbounded (billions of users and devices keep generating more and more data every second), unordered and unstructured. 

Big Data is about processing of peta bytes of data at rest. While analysis of data at rest is important due to possibilities inherent in batch style processing such as ability to try out different methodologies to a single problem, processing data on arrival allows us to react to changing conditions as changes happen. As a result of realtime or near realtime processing requirements, stream storage and processing solutions such as Apache Kafka, Apache Storm, Spark Streaming, Apache Samza and Apache Flink were created. Often refer to as Fast Data, this new set of technologies and applications are evolving rapidly. 

As more and more data sources are available for processing and analysis and with the advancement of Fast Data technologies, users have refined requirements such as event-time based ordering for windowed aggregations and joins and new windowing requirements such as session windows that defines the window based on data.

> At the same time, consumers of these datasets have evolved sophisticated requirements, such as event-time ordering and windowing by features of the data themselves, in addition to an insatiable hunger for faster answers. Meanwhile, practicality dictates that one can never fully optimize along all dimensions of cor- rectness, latency, and cost for these types of input. As a result, data processing practitioners are left with the quandary of how to reconcile the tensions between these seemingly competing propositions, often resulting in disparate implementations and systems.

In [The Data Flow Model](http://www.vldb.org/pvldb/vol8/p1792-Akidau.pdf), Akidau et al. propose a programming abstraction based on dataflow model that can be use to express refined data processing requirements mentioned above. Proposed abstraction is general enough to achieve balance between, correctness, latency and cost based on different implementation strategies. The abstractions proposed in this paper is available as a open source project with several backend implementations including Flink based implementation with SDKs in Java and Python.

## Main Contributions and Motivation

## Compare this with existing work

## Compare this with Streaming SQL